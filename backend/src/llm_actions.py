#!/usr/bin/python
# coding: utf-8

__author__ = 'PhucLeo <phuc@vnd.sateraito.co.jp>'
'''
@file: llm_actions.py
@brief: LLM Actions API

@since: 2025-09-03
@version: 1.0.0
@author: PhucLeo
'''

from flask import Response, stream_with_context
import requests
import json
import random

from google.appengine.api import namespace_manager

# Realtime Database
from firebase_admin import db

import sateraito_inc
import sateraito_func
import sateraito_page
from sateraito_page import Handler_Basic_Request, _BasePage
from sateraito_ai.perplexity import PerplexityAI

from sateraito_logger import logging
from sateraito_db import GoogleAppsDomainEntry, LLMConfiguration, OperationLog, LLMUsageLog

from sateraito_inc import flask_docker
if flask_docker:
	# import memcache, taskqueue
	pass
else:
	# from google.appengine.api import memcache, taskqueue
	pass

class LLMActionAPI(Handler_Basic_Request, _BasePage):

	def get_llm_configuration(self):
		"""
		Get LLM configuration from the database.
		Returns:
			dict: LLM configuration dictionary.
		"""
		try:
			llm_config_dict = LLMConfiguration.getDict()
			if not llm_config_dict:
				return None
			
			model_name = llm_config_dict.get('model_name')
			system_prompt = llm_config_dict.get('system_prompt')
			search_context_size = llm_config_dict.get('response_length_level', 'medium')

			return {
				'model_name': model_name,
				'system_message': system_prompt,
				'search_context_size': search_context_size,
			}
		
		except Exception as e:
			logging.exception('Error in get_llm_configuration: %s', str(e))
			return None

	def process_stream(self, unique_id, stream_path, question, generated_content_func):
		"""
		Stream the generated content to the client.
		Use firebase Realtime Database to store the stream data.
		Args:
			unique_id (str): Unique ID for the stream.
			stream_path (str): Path in Realtime Database to store the stream data.
			generated_content_func (function): Function that generates the content to be streamed.
		"""

		# Stream the generated content
		full_answer = ''
		metadata = {}
		
		try:
			# Create a reference to the stream path in Realtime Database
			stream_ref = db.reference(stream_path)

			# Initialize the stream data
			if not stream_ref.get():
				# Initialize the stream data
				stream_ref.set({
					'id': unique_id,
					'question': question,
					'answer': '',
					'created_at': sateraito_func.get_current_timestamp_ms(),
					'status': 'started'
				})

			max_chunk_size_wait_to_save_opt = [10, 15, 20]
			max_chunk_size_wait_to_save = max_chunk_size_wait_to_save_opt[1]
			chunk_count = 0

			for chunk in generated_content_func():
				if chunk:
					_id = chunk.get('id')
					_data = chunk.get('data')
					_event = chunk.get('event', 'message')
					
					if _event == 'message':
						chunk_count += 1

						full_answer += _data

						if chunk_count >= max_chunk_size_wait_to_save:
							# Update the answer in Realtime Database
							stream_ref.update({
								'answer': full_answer,
								'status': 'in_progress'
							})
							chunk_count = 0
							max_chunk_size_wait_to_save = max_chunk_size_wait_to_save_opt[random.randint(0, len(max_chunk_size_wait_to_save_opt)-1)]

					elif _event == 'metadata':
						# Handle metadata if needed
						metadata = _data
						stream_ref.update({
							'metadata': _data
						})

			# Finalize the stream data
			stream_ref.update({
				'status': 'completed',
				'completed_at': sateraito_func.get_current_timestamp_ms()
			})

			return True, 'completed', '', full_answer, metadata

		except Exception as e:
			logging.exception('Error in process_stream: %s', str(e))
			# Update the stream data with error status
			stream_ref.update({
				'status': 'error',
				'error': str(e),
				'completed_at': sateraito_func.get_current_timestamp_ms()
			})
			return False, 'error', str(e), full_answer, metadata

	def process_save_log(self, tenant, app_id, unique_id, stream_path, prompt, llm_config, answer, metadata, status, error_message=''):
		"""
		Save the LLM operation log to the database.
		Args:
			tenant (str): Tenant ID.
			app_id (str): Application ID.
			unique_id (str): Unique ID for the operation.
			stream_path (str): Stream path in Realtime Database.
			prompt (str): The prompt sent to the LLM.
			llm_config (dict): The LLM configuration used.
			answer (str): The answer generated by the LLM.
			metadata (dict): Additional metadata from the LLM response.
			status (str): Status of the operation ('completed', 'error', etc.).
			error_message (str): Error message if any.
		"""
		try:

			# Convert metadata to string if it's a dict
			metadata_str = metadata
			if isinstance(metadata_str, dict):
				metadata_str = json.dumps(metadata_str, ensure_ascii=False)

			# Save operation log
			model_name = llm_config.get('model_name', '') if llm_config else ''
			system_prompt = llm_config.get('system_prompt', '') if llm_config else ''
			response_length = len(answer) if answer else 0
			log_entry = OperationLog.save(tenant=tenant, app_id=app_id, client_domain=self.client_website_domain, unique_id=unique_id, stream_path=stream_path, prompt=prompt,
				model_name=model_name, system_prompt=system_prompt,
				response=answer, metadata=metadata_str, response_length=response_length,
				status=status, error_message=error_message
			)

			# Save usage log
			prompt_length = len(prompt)
			completion_length = len(answer)
			total_length = prompt_length + completion_length
			usage_metadata = ''
			if metadata and isinstance(metadata, dict) and 'usage' in metadata:
				usage_metadata = json.dumps(metadata['usage'], ensure_ascii=False)
			LLMUsageLog.save(tenant, app_id, self.client_website_domain, unique_id, stream_path, model_name, prompt_length, completion_length, total_length, usage_metadata)

			# Update LLM quota used
			increment_value = 1  # Each operation counts as 1
			GoogleAppsDomainEntry.incrementLLMQuotaUsed(tenant, increment_value)
			
			# Create a reference to the operation log
			path_save_log = f'{tenant}/{app_id}/operation-logs/{unique_id}'
			operation_ref = db.reference(sateraito_func.convert_path_real_time_firebase_database(path_save_log))
			operation_ref.set({
				'log_id': log_entry.key.id(),
				'created_at': sateraito_func.get_current_timestamp_ms()
			})
		
		except Exception as e:
			logging.exception('Error in process_save_log: %s', str(e))
			return False
		
		return True

	def check_llm_quota(self, tenant):
		"""
		Check if the LLM quota is available for the tenant.
		Args:
			tenant (str): Tenant ID.
		Returns:
			bool: True if quota is available, False otherwise.
		"""

		try:
			return GoogleAppsDomainEntry.isLLMQuotaExceeded(tenant) == False
		except Exception as e:
			logging.exception('Error in check_llm_quota: %s', str(e))
			return False

# Get method
class _ActionLLMSearchWeb(LLMActionAPI):

	def process(self, tenant, app_id):
		try:
			# Get headers
			unique_id = self.request.headers.get('X-Stream-ID')
			stream_path = self.request.headers.get('X-Stream-Path')
			# Get params
			prompt = self.request.json.get('query')

			if not sateraito_func.is_valid_unique_id(unique_id):
				return self.json_response({'message': 'bad_request'}, status=400)
			if not sateraito_func.is_valid_stream_path(stream_path):
				return self.json_response({'message': 'bad_request'}, status=400)
			if not prompt:
				return self.json_response({'message': 'bad_request'}, status=400)
			
			llm_config = self.get_llm_configuration()
			if not llm_config:
				return self.json_response({'message': 'llm_configuration_not_found'}, status=500)
			
			perplexity_ai = PerplexityAI()
			generate, citations = perplexity_ai.chat_completion(prompt, stream=True, **llm_config)
			
			# Process stream
			success, status, error_message, answer, metadata = self.process_stream(unique_id, stream_path, prompt, generate)
	
			# Save log
			self.process_save_log(tenant, app_id, unique_id, stream_path, prompt, llm_config, answer, metadata, status, error_message)

			if success:
				return self.json_response({
					'message': 'stream_completed',
					'answer': answer,
					'metadata': metadata
				}, status=200)
			else:
				return self.json_response({'message': 'stream_error'}, status=500)
		
		except Exception as e:
			logging.exception('Error in GetLLMConfiguration.process: %s', str(e))
			return self.json_response({'message': 'internal_server_error'}, status=500)

class ClientActionLLMSearchWeb(_ActionLLMSearchWeb):

	def post(self, tenant, app_id):
		# set namespace
		namespace_manager.set_namespace(tenant)

		if not self.verifyBearerToken(tenant):
			return self.json_response({'message': 'forbidden'}, status=403)
		
		if not self.check_llm_quota(tenant):
			return self.json_response({'message': 'llm_quota_exceeded'}, status=403)
		
		return self.process(tenant, app_id)


# For dev
class ResetLLMQuotaUsed(Handler_Basic_Request, _BasePage):

	def get(self, tenant):
		try:
			# set namespace
			namespace_manager.set_namespace(tenant)

			# GoogleAppsDomainEntry
			row = GoogleAppsDomainEntry.getInstance(tenant)
			row.llm_quota_used = 0
			row.put()

			# Delete path in Realtime Database
			path = f'/{tenant}'
			ref = db.reference(sateraito_func.convert_path_real_time_firebase_database(path))
			ref.delete()

			# Clear logs
			q = OperationLog.query()
			ops = q.fetch(1000)
			for op in ops:
				op.key.delete()

			q = LLMUsageLog.query()
			usages = q.fetch(1000)
			for usage in usages:
				usage.key.delete()

			return self.json_response({'message': 'ok'}, status=200)
		
		except Exception as e:
			logging.exception('Error in ResetLLMQuotaUsed.post: %s', str(e))
			return self.json_response({'message': 'internal_server_error'}, status=500)


def add_url_rules(app):
	app.add_url_rule('/<string:tenant>/<string:app_id>/client/llm-actions/search-web', methods=['POST'],
									view_func=ClientActionLLMSearchWeb.as_view('ClientActionLLMSearchWeb'))

	app.add_url_rule('/<string:tenant>/reset-llm-quota-used', methods=['GET'],
									view_func=ResetLLMQuotaUsed.as_view('ResetLLMQuotaUsed'))

